<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PPO in post training | Xuan's Random Thoughts </title> <meta name="author" content="Xuan Zhang"> <meta name="description" content="learning RLHF"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhangxuan1918.github.io/blog/2025/rlhf/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Xuan's Random Thoughts </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PPO in post training</h1> <p class="post-meta"> Created on July 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rlhf"> <i class="fa-solid fa-hashtag fa-sm"></i> rlhf,</a>   <a href="/blog/tag/ppo"> <i class="fa-solid fa-hashtag fa-sm"></i> ppo</a>   ·   <a href="/blog/category/tech-posts"> <i class="fa-solid fa-tag fa-sm"></i> tech-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This is a blog to record my learning of PPO. I have been working on RLHF for a while. But I always feel I cannot connect all dots together to have a good overall understanding. This blog is to record my understanding and will be used for my own reference. I’m lazy so I didn’t include the references I read.</p> <h1 id="high-level-understanding">High Level Understanding</h1> <p>On the high level, the actor generates responses and reward model gives rewards. For responses with high rewards, the actor increases the probabiliy. For responses with low rewards, the actor decreases the probabiliy.</p> <p>So, the objective is roughly</p> <p>\(\begin{align} \max_{\theta}\sum_{\tau\sim\pi_{\theta}}[R({\tau})\mathbb{P}(\tau;\theta)] \end{align}\) Where $\tau$ is a samppled trajectory.</p> <p>Intuitively, in order to maximize this objective</p> <ul> <li>If the reward $R({\tau})$ is positive, we should increase $\mathbb{P}(\tau;\theta)$</li> <li>If the reward $R({\tau})$ is negative, we should decrease $\mathbb{P}(\tau;\theta)$</li> </ul> <p>Note the differences between SFT and RL are</p> <ul> <li>SFT: <ol> <li>Immitation: model tries to immitate the given (positive) targets</li> <li>offline: targets $\mathbb{D}$ are generated offline by human or other models</li> <li>SFT’s objective is to $\max_{\theta}\sum_{\tau\sim\mathbb{D}}\mathbb{P}(\tau;\theta)$</li> </ol> </li> <li>RL: <ol> <li>online: actor generates rollouts and reward model gives rewards</li> <li>rewards can be negative and positive</li> <li>actor tries to increase the probability of rollouts with high rewards while decrease the probability of rollouts with low rewards</li> </ol> </li> </ul> <h1 id="policy-gradient">Policy Gradient</h1> <p>Starting with policy gradient, the goal is to maximize the expected return</p> \[\begin{align} J(\pi_{\theta}) = \mathbb{E}_{\tau\sim\pi_{\theta}}[R(\tau)] \end{align}\] <p>Where we sample trajectory $\tau$ from actor $\pi_{\theta}$.</p> <p>Now, we can take the gradient of $J$</p> <p>\(\begin{align} \nabla_{\theta} J(\pi_{\theta}) &amp;= \nabla_{\theta}\mathbb{E}_{\tau\sim\pi_{\theta}}[R(\tau)] \\ &amp;= \nabla_{\theta}\sum_{\tau}R(\tau)P(\tau;\theta) \\ &amp;= \sum_{\tau}R(\tau)\nabla_{\theta}P(\tau;\theta) \\ &amp;= \sum_{\tau}R(\tau)\nabla_{\theta}\log P(\tau;\theta)P(\tau;\theta) \\ &amp;= \mathbb{E}_{\tau\sim \pi_{\theta}}[R(\tau)\nabla_{\theta}\log P(\tau;\theta)] \\ &amp;= \mathbb{E}_{\tau\sim \pi_{\theta}}[R(\tau)\sum_{t=0}^{\infty}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \\ &amp;= \sum_{t=0}^{\infty}\mathbb{E}_{a_{t}\sim \pi(\cdot|s_{t})}[R_{t}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \\ &amp;= \mathbb{E}_{t}[R_{t}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \end{align}\) where $R_{t}$ is the discounted reward starting from timestamp $t$.</p> <p>The counter intuitive part here is that we first derive the gradient. In order to use backprop to optimize, we need to fine a surrogate loss function which when taking gradient w.r.t. $\theta$, we get the exact same gradient.</p> <p>The loss function is defined as</p> \[\begin{align} \mathbb{L}_{pg}(\theta) &amp;= -\mathbb{E}_{t}[R_{t}\log \pi_{\theta}(a_{t}|s_{t})] \end{align}\] <p>It has high variance due to</p> <ol> <li>we only sample a few trajectories when computing the gradient</li> <li>we attribute response reward equally back to each token</li> </ol> <h1 id="reinforce">REINFORCE</h1> <p>To reduce the variance, we can substract a baseline from the reward.</p> <p>\(\begin{align} \nabla_{\theta} J(\pi_{\theta}) &amp;= \mathbb{E}_{t}[(R_{t}-b_{t})\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \end{align}\) Where $b(t)$ is usually the average reward across the batch. This is unbiased because $b(t)$ is indenpendent of $a_{t}$.</p> <p>\(\begin{align} \mathbb{E}_{t}[\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] &amp;= \sum_{a_{t}\sim\pi_{\theta}({\cdot|s_{t}})}\left[\frac{\nabla_{\theta}\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}\pi_{\theta}(a_{t}|s_{t})\right] \\ &amp;= \nabla_{\theta}\mathbb{E}_{t}[ \sum_{a_{t}\sim\pi_{\theta}({\cdot|s_{t}})}\pi_{\theta}(a_{t}|s_{t})]=\nabla_{\theta}1=0 \end{align}\)</p> <h1 id="ppo">PPO</h1> <h2 id="policy-loss">Policy Loss</h2> <p>We have following equation \(\begin{align} \mathbb{E}_{\tau\sim\pi}[R(\tau)] = \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi}(s_{0})] \end{align}\)</p> <p>where \(\mathbb{E}_{\tau\sim\pi}[R(\tau)]\) is the expected total reward from trajectories and \(\mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi}(s_{0})]\) is the expected value from starting state.</p> <p>Given an old policy $\pi_{\theta_{old}}$, we have following reward</p> <p>\(\begin{align} \mathbb{J}(\theta_{old}) &amp;= \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi_{\theta_{old}}}(s_{0})] \\ &amp;= \mathbb{E}_{s_{0}\sim\rho_{s}}[\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) - \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})] \\ &amp;= \mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) - \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})] \end{align}\) Because $s_{0}$ is independent of $\pi_{\theta}$.</p> <p>Now, suppose we come up a new policy $\pi_{\theta}$. The reward is \(\begin{align} \mathbb{J}(\theta) &amp;= \mathbb{E}_{\pi_{\theta}}[V^{\pi_{\theta}}(s_{0})] \\ &amp;= \mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty}\gamma^{t}r(a_{t}, s_{t})] \end{align}\)</p> <p>The reward difference between the new and old policies is</p> \[\begin{align} \mathbb{J}(\theta)-\mathbb{J}(\theta_{old}) &amp;= \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi_{\theta}}(s_{0})] - \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi_{\theta_{old}}}(s_{0})]\\ &amp;= \mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty}\gamma^{t}r(a_{t}, s_{t})]-\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) - \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})\right] \\ &amp;= \mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}r(a_{t}, s_{t})-\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) + \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})\right] \\ &amp;= \mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}(r(a_{t}, s_{t}) - V^{\pi_{\theta_{old}}}(s_{t}) + \gamma V^{\pi_{\theta_{old}}}(s_{t+1}))\right] \\ &amp;= \sum_{t=0}^{\infty}\mathbb{E}_{a_{t}\sim\pi_{\theta}(\cdot|s_{t})}\left[\gamma^{t}(r(a_{t}, s_{t}) - V^{\pi_{\theta_{old}}}(s_{t}) + \gamma V^{\pi_{\theta_{old}}}(s_{t+1}))\right] \\ &amp;= \mathbb{E}_{t}\left[\gamma^{t}(r(a_{t}, s_{t}) - V^{\pi_{\theta_{old}}}(s_{t}) + \gamma V^{\pi_{\theta_{old}}}(s_{t+1}))\right] \\ &amp;= \mathbb{E}_{t}\left[\gamma^{t}A(a_{t}, s_{t})\right] \end{align}\] <p>We would like to find a new policy $\pi_{\theta}$ such that \(\mathbb{E}_{t}\left[\gamma^{t}A(a_{t}, s_{t})\right]&gt;0\). In practice, instead of maximizing the global objective. PPO maximizes the local objective, a mini batch. So $\gamma^{t}$ is applied when computing advantages.</p> <p>Now, we have \(\begin{align} \max_{\theta}\mathbb{E}_{t}\left[A(a_{t}, s_{t})\right] \end{align}\)</p> <p>Since we take expectation w.r.t. $\pi_{\theta}$ and we sample trajectories from $\pi_{\theta_{old}}$. Using importance sampling, we get</p> <p>\(\begin{align} \max_{\theta}\mathbb{E}_{t}\left[r_{t}(\theta)A(a_{t}, s_{t})\right] \end{align}\) Where $r_{t}(\theta)=\frac{\pi_{\theta}(a_{t}/s_{t})}{\pi_{\theta_{old}(a_{t}/s_{t})}}$.</p> <p>To avoid large update, PPO also clips the ratio $r_{t}(\theta)$. The final PPO policy loss is \(\begin{align} \mathbb{L}_{ppo-policy}(\theta)=-\mathbb{E}_{t}\left[\min(r_{t}(\theta)A(a_{t}, s_{t}), clip(r_{t}(\theta), 1-\epsilon, 1+\epsilon)A(a_{t}, s_{t}))\right] \end{align}\)</p> <p>Another Hand-waving way to derive PPO policy is to use Policy Gradient loss funciton. Recall, the loss function is</p> \[\mathbb{L}_{pg}(\theta) = -\mathbb{E}_{t}[R_{t}\log \pi_{\theta}(a_{t}|s_{t})]\] <p>In genereal, we replace $R_{t}$ by $A_{t}$ to reduce variance. We also have</p> <p>\(\begin{align} \nabla_{\theta}\mathbb{L}_{pg}(\theta) &amp;= -\mathbb{E}_{t}[R_{t}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \\ &amp;= -\mathbb{E}_{t}[R_{t}\frac{\nabla_{\theta}\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}] \end{align}\) So we replace $\log \pi_{\theta}(a_{t}|s_{t})$ by $\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$.</p> <h2 id="gae">GAE</h2> <p>\(\begin{align} A(t) &amp;= \delta_{t} + \gamma \lambda A(t+1) \\ \delta_{t} &amp;= r(t) + \gamma V(s_{t+1}) - V(s_{t}) \end{align}\)</p> <ul> <li>If $\lambda=0$, we have $A(t)=\delta(t)$ which is time difference. This is high bias low variance.</li> <li>If $\lambda=1$, we have $A(t)=\delta(t) + \gamma A(t+1)$ which is monte carlo simulation using all trajectory. This is high variance low bias.</li> </ul> <p>GAE sits in between to balance the bias and variance.</p> <p>In implementation, each token has KL loss as reward. The last token recieves the reward. Then we compute advantages backwards.</p> <h2 id="kl-loss">KL Loss</h2> <p>KL loss is introduced in the advantage. By definition, KL divergence is computed over the full vocab</p> \[KL[\pi_{\theta}(\cdot|s_{t})||\pi_{ref}(\cdot|s_{t})] = \sum_{a_{t}\in V}\pi_{\theta}(a_{t}|s_{t})\log \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{ref}(a_{t}|s_{t})}\] <p>In practice, we only compute KL on sampled tokens as an approximation</p> \[KL_{approx} = \log \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{ref}(a_{t}|s_{t})}\] </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xuan Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>