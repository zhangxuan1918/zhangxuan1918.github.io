<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zhangxuan1918.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zhangxuan1918.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-16T12:17:53+00:00</updated><id>https://zhangxuan1918.github.io/feed.xml</id><title type="html">Xuan’s Random Thoughts</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">PPO in post training</title><link href="https://zhangxuan1918.github.io/blog/2025/rlhf/" rel="alternate" type="text/html" title="PPO in post training"/><published>2025-07-14T14:24:00+00:00</published><updated>2025-07-14T14:24:00+00:00</updated><id>https://zhangxuan1918.github.io/blog/2025/rlhf</id><content type="html" xml:base="https://zhangxuan1918.github.io/blog/2025/rlhf/"><![CDATA[<p>This is a blog to record my learning of PPO. I have been working on RLHF for a while. But I always feel I cannot connect all dots together to have a good overall understanding. This blog is to record my understanding and will be used for my own reference. I’m lazy so I didn’t include the references I read.</p> <h1 id="high-level-understanding">High Level Understanding</h1> <p>On the high level, the actor generates responses and reward model gives rewards. For responses with high rewards, the actor increases the probabiliy. For responses with low rewards, the actor decreases the probabiliy.</p> <p>So, the objective is roughly</p> <p>\(\begin{align} \max_{\theta}\sum_{\tau\sim\pi_{\theta}}[R({\tau})\mathbb{P}(\tau;\theta)] \end{align}\) Where $\tau$ is a samppled trajectory.</p> <p>Intuitively, in order to maximize this objective</p> <ul> <li>If the reward $R({\tau})$ is positive, we should increase $\mathbb{P}(\tau;\theta)$</li> <li>If the reward $R({\tau})$ is negative, we should decrease $\mathbb{P}(\tau;\theta)$</li> </ul> <p>Note the differences between SFT and RL are</p> <ul> <li>SFT: <ol> <li>Immitation: model tries to immitate the given (positive) targets</li> <li>offline: targets $\mathbb{D}$ are generated offline by human or other models</li> <li>SFT’s objective is to $\max_{\theta}\sum_{\tau\sim\mathbb{D}}\mathbb{P}(\tau;\theta)$</li> </ol> </li> <li>RL: <ol> <li>online: actor generates rollouts and reward model gives rewards</li> <li>rewards can be negative and positive</li> <li>actor tries to increase the probability of rollouts with high rewards while decrease the probability of rollouts with low rewards</li> </ol> </li> </ul> <h1 id="policy-gradient">Policy Gradient</h1> <p>Starting with policy gradient, the goal is to maximize the expected return</p> \[\begin{align} J(\pi_{\theta}) = \mathbb{E}_{\tau\sim\pi_{\theta}}[R(\tau)] \end{align}\] <p>Where we sample trajectory $\tau$ from actor $\pi_{\theta}$.</p> <p>Now, we can take the gradient of $J$</p> <p>\(\begin{align} \nabla_{\theta} J(\pi_{\theta}) &amp;= \nabla_{\theta}\mathbb{E}_{\tau\sim\pi_{\theta}}[R(\tau)] \\ &amp;= \nabla_{\theta}\sum_{\tau}R(\tau)P(\tau;\theta) \\ &amp;= \sum_{\tau}R(\tau)\nabla_{\theta}P(\tau;\theta) \\ &amp;= \sum_{\tau}R(\tau)\nabla_{\theta}\log P(\tau;\theta)P(\tau;\theta) \\ &amp;= \mathbb{E}_{\tau\sim \pi_{\theta}}[R(\tau)\nabla_{\theta}\log P(\tau;\theta)] \\ &amp;= \mathbb{E}_{\tau\sim \pi_{\theta}}[R(\tau)\sum_{t=0}^{\infty}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \\ &amp;= \sum_{t=0}^{\infty}\mathbb{E}_{a_{t}\sim \pi(\cdot|s_{t})}[R_{t}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \\ &amp;= \mathbb{E}_{t}[R_{t}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \end{align}\) where $R_{t}$ is the discounted reward starting from timestamp $t$.</p> <p>The counter intuitive part here is that we first derive the gradient. In order to use backprop to optimize, we need to fine a surrogate loss function which when taking gradient w.r.t. $\theta$, we get the exact same gradient.</p> <p>The loss function is defined as</p> \[\begin{align} \mathbb{L}_{pg}(\theta) &amp;= -\mathbb{E}_{t}[R_{t}\log \pi_{\theta}(a_{t}|s_{t})] \end{align}\] <p>It has high variance due to</p> <ol> <li>we only sample a few trajectories when computing the gradient</li> <li>we attribute response reward equally back to each token</li> </ol> <h1 id="reinforce">REINFORCE</h1> <p>To reduce the variance, we can substract a baseline from the reward.</p> <p>\(\begin{align} \nabla_{\theta} J(\pi_{\theta}) &amp;= \mathbb{E}_{t}[(R_{t}-b_{t})\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \end{align}\) Where $b(t)$ is usually the average reward across the batch. This is unbiased because $b(t)$ is indenpendent of $a_{t}$.</p> <p>\(\begin{align} \mathbb{E}_{t}[\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] &amp;= \sum_{a_{t}\sim\pi_{\theta}({\cdot|s_{t}})}\left[\frac{\nabla_{\theta}\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}\pi_{\theta}(a_{t}|s_{t})\right] \\ &amp;= \nabla_{\theta}\mathbb{E}_{t}[ \sum_{a_{t}\sim\pi_{\theta}({\cdot|s_{t}})}\pi_{\theta}(a_{t}|s_{t})]=\nabla_{\theta}1=0 \end{align}\)</p> <h1 id="ppo">PPO</h1> <h2 id="policy-loss">Policy Loss</h2> <p>We have following equation \(\begin{align} \mathbb{E}_{\tau\sim\pi}[R(\tau)] = \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi}(s_{0})] \end{align}\)</p> <p>where \(\mathbb{E}_{\tau\sim\pi}[R(\tau)]\) is the expected total reward from trajectories and \(\mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi}(s_{0})]\) is the expected value from starting state.</p> <p>Given an old policy $\pi_{\theta_{old}}$, we have following reward</p> <p>\(\begin{align} \mathbb{J}(\theta_{old}) &amp;= \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi_{\theta_{old}}}(s_{0})] \\ &amp;= \mathbb{E}_{s_{0}\sim\rho_{s}}[\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) - \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})] \\ &amp;= \mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) - \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})] \end{align}\) Because $s_{0}$ is independent of $\pi_{\theta}$.</p> <p>Now, suppose we come up a new policy $\pi_{\theta}$. The reward is \(\begin{align} \mathbb{J}(\theta) &amp;= \mathbb{E}_{\pi_{\theta}}[V^{\pi_{\theta}}(s_{0})] \\ &amp;= \mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty}\gamma^{t}r(a_{t}, s_{t})] \end{align}\)</p> <p>The reward difference between the new and old policies is</p> \[\begin{align} \mathbb{J}(\theta)-\mathbb{J}(\theta_{old}) &amp;= \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi_{\theta}}(s_{0})] - \mathbb{E}_{s_{0}\sim\rho_{s}}[V^{\pi_{\theta_{old}}}(s_{0})]\\ &amp;= \mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty}\gamma^{t}r(a_{t}, s_{t})]-\mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) - \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})\right] \\ &amp;= \mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}r(a_{t}, s_{t})-\sum_{t=0}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t}) + \sum_{t=1}^{\infty}\gamma^{t}V^{\pi_{\theta_{old}}}(s_{t})\right] \\ &amp;= \mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{\infty}\gamma^{t}(r(a_{t}, s_{t}) - V^{\pi_{\theta_{old}}}(s_{t}) + \gamma V^{\pi_{\theta_{old}}}(s_{t+1}))\right] \\ &amp;= \sum_{t=0}^{\infty}\mathbb{E}_{a_{t}\sim\pi_{\theta}(\cdot|s_{t})}\left[\gamma^{t}(r(a_{t}, s_{t}) - V^{\pi_{\theta_{old}}}(s_{t}) + \gamma V^{\pi_{\theta_{old}}}(s_{t+1}))\right] \\ &amp;= \mathbb{E}_{t}\left[\gamma^{t}(r(a_{t}, s_{t}) - V^{\pi_{\theta_{old}}}(s_{t}) + \gamma V^{\pi_{\theta_{old}}}(s_{t+1}))\right] \\ &amp;= \mathbb{E}_{t}\left[\gamma^{t}A(a_{t}, s_{t})\right] \end{align}\] <p>We would like to find a new policy $\pi_{\theta}$ such that \(\mathbb{E}_{t}\left[\gamma^{t}A(a_{t}, s_{t})\right]&gt;0\). In practice, instead of maximizing the global objective. PPO maximizes the local objective, a mini batch. So $\gamma^{t}$ is applied when computing advantages.</p> <p>Now, we have \(\begin{align} \max_{\theta}\mathbb{E}_{t}\left[A(a_{t}, s_{t})\right] \end{align}\)</p> <p>Since we take expectation w.r.t. $\pi_{\theta}$ and we sample trajectories from $\pi_{\theta_{old}}$. Using importance sampling, we get</p> <p>\(\begin{align} \max_{\theta}\mathbb{E}_{t}\left[r_{t}(\theta)A(a_{t}, s_{t})\right] \end{align}\) Where $r_{t}(\theta)=\frac{\pi_{\theta}(a_{t}/s_{t})}{\pi_{\theta_{old}(a_{t}/s_{t})}}$.</p> <p>To avoid large update, PPO also clips the ratio $r_{t}(\theta)$. The final PPO policy loss is \(\begin{align} \mathbb{L}_{ppo-policy}(\theta)=-\mathbb{E}_{t}\left[\min(r_{t}(\theta)A(a_{t}, s_{t}), clip(r_{t}(\theta), 1-\epsilon, 1+\epsilon)A(a_{t}, s_{t}))\right] \end{align}\)</p> <p>Another Hand-waving way to derive PPO policy is to use Policy Gradient loss funciton. Recall, the loss function is</p> \[\mathbb{L}_{pg}(\theta) = -\mathbb{E}_{t}[R_{t}\log \pi_{\theta}(a_{t}|s_{t})]\] <p>In genereal, we replace $R_{t}$ by $A_{t}$ to reduce variance. We also have</p> <p>\(\begin{align} \nabla_{\theta}\mathbb{L}_{pg}(\theta) &amp;= -\mathbb{E}_{t}[R_{t}\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})] \\ &amp;= -\mathbb{E}_{t}[R_{t}\frac{\nabla_{\theta}\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}] \end{align}\) So we replace $\log \pi_{\theta}(a_{t}|s_{t})$ by $\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$.</p> <h2 id="gae">GAE</h2> <p>\(\begin{align} A(t) &amp;= \delta_{t} + \gamma \lambda A(t+1) \\ \delta_{t} &amp;= r(t) + \gamma V(s_{t+1}) - V(s_{t}) \end{align}\)</p> <ul> <li>If $\lambda=0$, we have $A(t)=\delta(t)$ which is time difference. This is high bias low variance.</li> <li>If $\lambda=1$, we have $A(t)=\delta(t) + \gamma A(t+1)$ which is monte carlo simulation using all trajectory. This is high variance low bias.</li> </ul> <p>GAE sits in between to balance the bias and variance.</p> <p>In implementation, each token has KL loss as reward. The last token recieves the reward. Then we compute advantages backwards.</p> <h2 id="kl-loss">KL Loss</h2> <p>KL loss is introduced in the advantage. By definition, KL divergence is computed over the full vocab</p> \[KL[\pi_{\theta}(\cdot|s_{t})||\pi_{ref}(\cdot|s_{t})] = \sum_{a_{t}\in V}\pi_{\theta}(a_{t}|s_{t})\log \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{ref}(a_{t}|s_{t})}\] <p>In practice, we only compute KL on sampled tokens as an approximation</p> \[KL_{approx} = \log \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{ref}(a_{t}|s_{t})}\]]]></content><author><name></name></author><category term="tech-posts"/><category term="rlhf,"/><category term="ppo"/><summary type="html"><![CDATA[learning RLHF]]></summary></entry></feed>